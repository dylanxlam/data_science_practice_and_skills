{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **What is data preparation?**\n",
    "\n",
    "Data preparation is the process of preparing source data for efficient and accurate analysis. Data-preparation activities include removing missing values, formatting features uniformly, and appending related data from external sources. Data preparation is sometimes called data munging or data wrangling.\n",
    "\n",
    "Data wrangling has six steps.\n",
    "\n",
    "\n",
    "1. **Discovering**\n",
    "- Discovery, also called data exploration, familiarizes the data scientist with source data in preparation for subsequent steps.\n",
    "\n",
    "2. **Structuring**\t\n",
    "- Structuring data transforms features to uniform formats, units, and scales.\n",
    "\n",
    "3. **Cleaning**\t\n",
    "- Cleaning data removes or replaces missing and outlier data.\n",
    "\n",
    "4. **Enriching**\t\n",
    "- Enriching data derives new features from existing features and appends new data from external sources.\n",
    "\n",
    "5. **Validating**\t\n",
    "- Validating data verifies that the dataset is internally consistent and accurate.\n",
    "    \n",
    "6. **Publishing**\t\n",
    "- Publishing data makes the dataset available to other data scientists by storing data in a database, uploading data to the cloud, or distributing data files.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Extract, Transform, Load**\n",
    "***Extract, Transform, Load (ETL)*** is a process that extracts data from transactional databases, transforms the data, and loads the data into an analytic database. ETL transforms data in a staging area, such as a temporary database, prior to loading data to the analytic database. \n",
    "\n",
    "***Extract, Load, Transform (ELT)*** is a variant of ETL that loads raw data directly to the analytic database and transforms the data in place.\n",
    "\n",
    "ETL is similar to data wrangling. Both processes structure, clean, enrich, and publish data. However, data wrangling is usually an informal process, executed manually by data scientists on a static dataset. ETL is an automated process that repeatedly extracts new data from transactional databases. ETL is usually applied to larger data volumes and more sources than data wrangling.\n",
    "\n",
    "ETL tools, also called data integration tools, extract and merge data from many different database systems. In principle, ETL tools can be used for data wrangling. However, because data wrangling is often manual and ad-hoc, data scientists usually prefer spreadsheets or programming languages such as Python, R, and SQL.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Preparation with Python**\n",
    "***pandas*** is a Python package that supports data wrangling. ***DataFrame*** is a pandas class that stores and manipulates datasets. In this material, ***dataframe***, in lowercase, refers to a DataFrame object.\n",
    "\n",
    "Dataframes consist of rows and columns, representing dataset instances and features. Each column has a data type. Rows and columns are identified by integer or string ***labels***. The set of row labels is called the ***index*** and the set of column labels is called ***columns***. Usually, row labels are automatically generated integers and column labels are manually specified strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Preparation with pandas**\n",
    "\n",
    "Method\t|  Parameters   |   Description\n",
    "\n",
    "**read_csv()**\n",
    "- filepath_or_buffer, sep=NoDefault.no_default\t\n",
    "- Returns a dataframe constructed from a CSV file. filepath_or_buffer is a string containing the full path for the CSV file. When the file is in the same directory as the code, only the file name is needed. sep specifies the character that separates values in the CSV file.\n",
    "\n",
    "**read_excel()**\t\n",
    "- io, sheet_name=0\t\n",
    "- Returns a dataframe constructed from an Excel spreadsheet. io is a string containing the full path for the Excel file. When the file is in the same directory as the code, only the file name is needed. sheet_name is a string or integer that specifies which Excel sheet to read.\n",
    "\n",
    "**read_sql_table()**\t\n",
    "- table_name, con, schema=None, columns=none\t\n",
    "- Returns a dataframe constructed from an SQL table. table_name specifies the table name. con specifies a database server connection string. schema specifies the schema in the database server. columns specifies which table columns to include in the dataframe.\n",
    "\n",
    "**DataFrame()**\t\n",
    "- data=None, index=None, columns=None\n",
    "- Returns a new dataframe. data specifies dataframe values as an array, dictionary, or another dataframe. index and columns specify row and column labels. The defaults index=None and columns=None generate integer labels.\n",
    "\n",
    "**dataframe.at[]**\t\n",
    "- index, column\t\n",
    "- Returns the dataframe value stored at index and column.\n",
    "\n",
    "**dataframe.info()**\n",
    "- verbose=None\t\n",
    "- Returns information about dataframe, such as number of rows and columns, data types, and memory usage. If verbose=False, shows only summary dataframe information and hides column details.\n",
    "\n",
    "**dataframe.loc[]**\t\n",
    "- indexRange, columnRange\t\n",
    "- Returns a slice of dataframe. indexRange specifies rows in the slice, as startIndex:endIndex. columnRange specifies columns in the slice as startLabel:endLabel.\n",
    "\n",
    "**dataframe.sort_values()**\t\n",
    "- by, axis=0, ascending=True, inplace=False\t\n",
    "- Sorts dataframe columns or rows. by specifies indexes or labels on which to sort. axis specifies whether to sort rows (0) or columns (1). ascending specifies whether to sort ascending or descending. inplace specifies whether to sort dataframe or return a new dataframe.\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Manipulation: Comparison of Groups**\n",
    "\n",
    "The first step of the data wrangling process is data discovery, or exploring patterns and trends within a dataset. Data exploration can be done visually through plots or figures, or numerically by comparing descriptive statistics.\n",
    "\n",
    "***Data manipulation*** is the process of organizing or subsetting a dataset to explore a research problem. Data manipulation is used to split datasets into multiple groups based on a categorical feature, or compare values of a dataset according to a specific condition. After data manipulation, descriptive statistics like the mean, median, or proportion can be calculated and compared across groups or conditions.\n",
    "\n",
    "Ex.\n",
    "1. The United Nations has several programs designed to increase access to education. One measure of educational access is the average years in school.\n",
    "2. GDP and EducationYears are unavailable for Brazil. Since data is missing, Brazil is filtered, or ignored, during data manipulation.\n",
    "3. The mean, or average, EducationYears for the five countries in the dataset with data available is (4.7 + 8.5 + 5.7 + 14.2 + 13.5) / 5 = 9.3 years.\n",
    "4. However, differences exist between countries, which affect access to education. Countries in the same continent or region are more likely to have similar education structures.\n",
    "5. **Grouping by continent highlights regional differences in education. Based on this data, students in Asia tend to spend less time in school than students in Europe and North America.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Grouping Data**\n",
    "***Grouping*** is used to separate a dataframe into subsets based on levels of a categorical feature. In some cases, a different analysis or model may be applied to each group. In other cases, grouping may be a temporary operation for calculating group sizes or descriptive statistics like group means. A ***frequency table*** is a table containing group sizes for a categorical feature.\n",
    "\n",
    "Ex. \n",
    "1. A data scientist would like to compare the mean years of schooling (Years) for each continent.\n",
    "2. The dataset is grouped into three subsets: one subset for Africa, one subset for the Americas, and one subset Asia.\n",
    "3. After grouping, the mean is calculated for each subset.\n",
    "4. Means for each continent are combined into a single table. Based on the table, countries in Africa have the lowest mean years in school, then Asia, then the Americas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pivot Tables**\n",
    "\n",
    "A ***pivot table*** calculates and displays descriptive statistics after grouping based on values of two categorical features. One categorical feature is assigned to the pivot table's rows, and another categorical feature is assigned to the columns. A ***contingency table*** is a special case of a pivot table in which the descriptive statistic is the number of instances in each combination of categorical features.\n",
    "\n",
    "ex.\n",
    "\n",
    "1. Pivot tables provide the number of instances that share a combination of two categorical features.\n",
    "2. The row feature's unique values are listed on the pivot table's rows.\n",
    "3. The column feature's unique values are listed on the pivot table's columns.\n",
    "4. A descriptive statistic like the number of instances is added to each row/column combination. Ex: 26 European countries have high internet access. 13 Asian countries have low internet access.\n",
    "5. Pivot tables may contain descriptive statistics for additional features. Ex: The mean years in school for European countries with high internet access is 11.6 years, and the mean for Asian countries with low internet access is 6.3 years.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Manipulation with Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Years</th>\n",
       "      <th>Internet access</th>\n",
       "      <th>Emissions range</th>\n",
       "      <th>Fertility</th>\n",
       "      <th>Emissions</th>\n",
       "      <th>Internet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Asia</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.254</td>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Europe</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Low</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.590</td>\n",
       "      <td>65.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Africa</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2.64</td>\n",
       "      <td>3.690</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Angola</td>\n",
       "      <td>Africa</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1.120</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>Americas</td>\n",
       "      <td>9.9</td>\n",
       "      <td>High</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2.26</td>\n",
       "      <td>4.410</td>\n",
       "      <td>77.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Uruguay</td>\n",
       "      <td>Americas</td>\n",
       "      <td>8.7</td>\n",
       "      <td>High</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.010</td>\n",
       "      <td>80.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Uzbekistan</td>\n",
       "      <td>Asia</td>\n",
       "      <td>11.5</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2.23</td>\n",
       "      <td>2.810</td>\n",
       "      <td>55.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Asia</td>\n",
       "      <td>8.2</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.160</td>\n",
       "      <td>69.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>Africa</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>4.87</td>\n",
       "      <td>0.302</td>\n",
       "      <td>14.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Africa</td>\n",
       "      <td>8.2</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0.850</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Country Continent  Years Internet access Emissions range  Fertility  \\\n",
       "0    Afghanistan      Asia    3.8             Low             Low       4.33   \n",
       "1        Albania    Europe   10.0        Moderate             Low       1.71   \n",
       "2        Algeria    Africa    8.0             Low        Moderate       2.64   \n",
       "3         Angola    Africa    5.1             Low             Low       5.55   \n",
       "4      Argentina  Americas    9.9            High        Moderate       2.26   \n",
       "..           ...       ...    ...             ...             ...        ...   \n",
       "146      Uruguay  Americas    8.7            High        Moderate       1.97   \n",
       "147   Uzbekistan      Asia   11.5        Moderate        Moderate       2.23   \n",
       "148      Vietnam      Asia    8.2        Moderate        Moderate       1.95   \n",
       "149       Zambia    Africa    7.0             Low             Low       4.87   \n",
       "150     Zimbabwe    Africa    8.2             Low             Low       3.61   \n",
       "\n",
       "     Emissions  Internet  \n",
       "0        0.254      16.8  \n",
       "1        1.590      65.4  \n",
       "2        3.690      49.0  \n",
       "3        1.120      29.0  \n",
       "4        4.410      77.7  \n",
       "..         ...       ...  \n",
       "146      2.010      80.7  \n",
       "147      2.810      55.2  \n",
       "148      2.160      69.8  \n",
       "149      0.302      14.3  \n",
       "150      0.850      25.0  \n",
       "\n",
       "[151 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import data and display\n",
    "country = pd.read_csv('/Users/dylanlam/Documents/GitHub/data_science_practice_and_skills/datasets/country_complete.csv')\n",
    "country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Internet access</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Moderate</th>\n",
       "      <th>Very high</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Continent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Africa</th>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Americas</th>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asia</th>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Europe</th>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oceania</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Internet access  High   Low  Moderate  Very high\n",
       "Continent                                       \n",
       "Africa            1.0  36.0       8.0        NaN\n",
       "Americas          8.0   7.0      10.0        1.0\n",
       "Asia             10.0  13.0       9.0        8.0\n",
       "Europe           26.0   NaN       3.0        7.0\n",
       "Oceania           2.0   1.0       1.0        NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorical features are sorted in alphabetical order by default\n",
    "# np.size counts the number of entries\n",
    "country['Internet access'] = country['Internet access'].astype('category')\n",
    "country.pivot_table(\n",
    "    values='Years', index='Continent', columns='Internet access', aggfunc=np.size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Internet access</th>\n",
       "      <th>Low</th>\n",
       "      <th>Moderate</th>\n",
       "      <th>High</th>\n",
       "      <th>Very high</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Continent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Africa</th>\n",
       "      <td>36.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Americas</th>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asia</th>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Europe</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oceania</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Internet access   Low  Moderate  High  Very high\n",
       "Continent                                       \n",
       "Africa           36.0       8.0   1.0        NaN\n",
       "Americas          7.0      10.0   8.0        1.0\n",
       "Asia             13.0       9.0  10.0        8.0\n",
       "Europe            NaN       3.0  26.0        7.0\n",
       "Oceania           1.0       1.0   2.0        NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cat.reorder_categories is useful for rearranging the order\n",
    "# (ex: low to high)\n",
    "country['Internet access'] = country['Internet access'].cat.reorder_categories(\n",
    "    ['Low', 'Moderate', 'High', 'Very high']\n",
    ")\n",
    "# Display the number of countries in a pivot table of continent and\n",
    "# internet access\n",
    "country.pivot_table(\n",
    "    values='Years', index='Continent', columns='Internet access', aggfunc=np.size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Internet access</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Belize</td>\n",
       "      <td>Americas</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bolivia</td>\n",
       "      <td>Americas</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>El Salvador</td>\n",
       "      <td>Americas</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Guatemala</td>\n",
       "      <td>Americas</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Haiti</td>\n",
       "      <td>Americas</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Honduras</td>\n",
       "      <td>Americas</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Nicaragua</td>\n",
       "      <td>Americas</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Country Continent Internet access\n",
       "14        Belize  Americas             Low\n",
       "17       Bolivia  Americas             Low\n",
       "43   El Salvador  Americas             Low\n",
       "56     Guatemala  Americas             Low\n",
       "59         Haiti  Americas             Low\n",
       "60      Honduras  Americas             Low\n",
       "101    Nicaragua  Americas             Low"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which 7 countries in the Americas have low Internet access?\n",
    "country[(country['Continent'] == 'Americas') & (country['Internet access'] == 'Low')][['Country', 'Continent', 'Internet access']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Structuring Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Formatting Data**\n",
    "\n",
    "All data within a single column and similar data in multiple columns should be stored in a uniform format. Ex:\n",
    "\n",
    "All dates and times might be stored as the datetime data type, with a 24-hour clock in Coordinated Universal Time (UTC).\n",
    "All lengths might be stored as meters.\n",
    "All percentages might be stored as decimal values between 0 and 1, rather than integers between 0 and 100.\n",
    "All names of people might be stored as 'FirstName LastName' with no prefix, suffix, or middle initial.\n",
    "A uniform storage format facilitates aggregating and comparing data within and across columns. Standardizing the storage format also minimizes analysis errors.\n",
    "\n",
    "Although storage formats should be uniform, display formats may vary according to the audience. Ex: In the animation below, Gross Domestic Product (GDP) is stored as integer dollars but might be displayed as billions of dollars.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature Scaling**\n",
    "\n",
    "The numeric features in a dataset often have different scales. In some datasets, scales may differ by orders of magnitude. Many algorithms execute faster or generate better results when scales are similar or identical. ***Feature scaling*** converts numeric features to uniform ranges. Two common feature scaling methods are ***standardization*** and normalization.\n",
    "\n",
    "Standardization converts features to a range centered at 0, with 1 representing a standard deviation:\n",
    "\n",
    " x(standardized) = (ORIGINALx - MEANx)/(SDx)\n",
    " \n",
    "(μx) is the mean and (σx) is the standard deviation of feature. The standardized value is called a z-score. Since each unit represents one standard deviation, most z-scores fall between -2 and 2.\n",
    "\n",
    "***Normalization*** converts features to the range [0,1]:\n",
    "\n",
    "x(normalized) = (xoriginal - MINx)/(MAXx - MINx)\n",
    "\n",
    "Standardization is usually preferred over normalization, since standardization positions values relative to the mean and standard deviation. Normalization is useful when algorithms require all features on identical scales.\n",
    "\n",
    "Standardization is best when outliers are present. Standardized values are not skewed by outliers, but most normalized values are compressed into a small range.\n",
    "\n",
    "***Terminology***\n",
    "\n",
    "- Feature scaling terminology varies. Standardization is sometimes called z-score normalization. Normalization is sometimes called min-max scaling.\n",
    "\n",
    "Ex. \n",
    "1. The housing dataset has the features Price and Age. The feature scales differ by orders of magnitude.\n",
    "2. Standardized Price values are computed from the mean, 170,590, and standard deviation, 74,010.\n",
    "3. Standardized Age values are computed from the mean, 21.2, and standard deviation, 5.8.\n",
    "4. Normalized Price values are computed from the minimum, 90,300, and maximum, 269,500.\n",
    "5. Normalized Age values are computed from the minimum, 14, and maximum, 28.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Structuring Data with Python**\n",
    "\n",
    "The Python language, and the pandas and sklearn packages, have many data structuring methods. Selected methods and functions that format, scale, and unpack data are described in the tables below. The tables include all required parameters and important optional parameters, but exclude infrequently used optional parameters.\n",
    "\n",
    "string[start:end] is not a method but is useful for unpacking string columns and therefore included in the table.\n",
    "\n",
    "Methods that change data contain an optional copy parameter. If copy is True, changes are returned in a new dataframe or array. If copy is False, changes are made to the input dataframe or array.\n",
    "\n",
    "##### **Python data structuring methods.**\n",
    "\n",
    "**Method\tParameters\tDescription**\n",
    "\n",
    "***string[start:end]***\t\n",
    "- none\t\n",
    "- Returns the substring of string that begins at the index start and ends at the index end - 1.\n",
    "\n",
    "***string.capitalize(), string.upper(), string.lower(), string.title()***\t\n",
    "- none\t\n",
    "- Returns a copy of string with the initial character uppercase, all characters uppercase, all characters lowercase, or the initial character of all words uppercase.\n",
    "\n",
    "***to_datetime()***\t\n",
    "- arg\t\n",
    "- Converts arg to datetime data type and returns the converted object. Data type of arg may be int, float, str, datetime, list, tuple, one-dimensional array, Series, or DataFrame.\n",
    "\n",
    "***to_numeric()***\t\n",
    "- arg\t\n",
    "- Converts arg to numeric data type and returns the converted object. Data type of arg may be scalar, list, tuple, one-dimensional array, or Series.\n",
    "\n",
    "\n",
    "##### **Pandas data structuring methods**\n",
    "\n",
    "**Method\tParameters\tDescription**\n",
    "\n",
    "***df.astype()***\t\n",
    "- dtype, copy=True\t\n",
    "- Converts the data type of all dataframe df columns to dtype. To alter individual columns, specify dtype as {col: dtype, col:dtype, . . .}.\n",
    "\n",
    "***df.insert()***\t\n",
    "- loc, column, value\t\n",
    "- Inserts a new column with label column at location loc in dataframe df. value is a Scalar, Series, or Array of values for the new column.\n",
    "\n",
    "###### **sklearn data structuring methods**\n",
    "\n",
    "**Method\tParameters\tDescription**\n",
    "\n",
    "***preprocessing.scale()***\n",
    "- X, axis=0, with_mean=True, with_std=True, copy=True\t\n",
    "- Standardizes data in input X of data type Array or DataFrame. axis indicates whether to standarize along columns (0) or rows (1). with_mean=True centers the data at the mean value. with_std=True scales the data so that one represents a standard deviation.\n",
    "\n",
    "***preprocessing.MinMaxScaler().fit_transform()***\t\n",
    "- feature_range=(0,1), copy=True, X\t\n",
    "- Normalizes data in input X, a fit_transform() parameter of data type Array or DataFrame. feature_range specifies the range of scaled data. feature_range and copy are MinMaxScaler() parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90300</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150500</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>269500</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98000</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>244650</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Price  Age\n",
       "0   90300   14\n",
       "1  150500   27\n",
       "2  269500   22\n",
       "3   98000   15\n",
       "4  244650   28"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "data = {'Price': [90300, 150500, 269500, 98000, 244650],\n",
    "        'Age': [14, 27, 22, 15, 28]}\n",
    "\n",
    "original = pd.DataFrame(data)\n",
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.084852</td>\n",
       "      <td>-1.231895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.271449</td>\n",
       "      <td>0.992360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.336439</td>\n",
       "      <td>0.136877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.980812</td>\n",
       "      <td>-1.060798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000674</td>\n",
       "      <td>1.163456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Price       Age\n",
       "0 -1.084852 -1.231895\n",
       "1 -0.271449  0.992360\n",
       "2  1.336439  0.136877\n",
       "3 -0.980812 -1.060798\n",
       "4  1.000674  1.163456"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize dataframe and return as an array\n",
    "standardizedArray = preprocessing.scale(original)\n",
    "\n",
    "# Convert standardized array to dataframe 'standardized'\n",
    "standardized = pd.DataFrame(standardizedArray, columns=[\"Price\", \"Age\"])\n",
    "standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.335938</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.861328</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Price       Age\n",
       "0  0.000000  0.000000\n",
       "1  0.335938  0.928571\n",
       "2  1.000000  0.571429\n",
       "3  0.042969  0.071429\n",
       "4  0.861328  1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize dataframe and return as an array\n",
    "normalizedArray = preprocessing.MinMaxScaler().fit_transform(original)\n",
    "\n",
    "# Convert normalized array to dataframe 'normalized'\n",
    "normalized = pd.DataFrame(normalizedArray, columns=[\"Price\", \"Age\"])\n",
    "normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cleaning Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dirty Data**\n",
    "\n",
    "Raw datasets often contain missing, outlier, and duplicate data.\n",
    "\n",
    "- ***Missing data*** is an unknown or inapplicable value. In a database, missing data is represented as NULL. In Python, missing data is represented as NaN (not a number), NaT (not a time), None (an unspecified object), or a blank value.\n",
    "\n",
    "- ***Outlier data*** is a numeric value that is much larger or smaller than other values in the same feature. Outlier data is usually defined as two or three standard deviations from the feature mean.\n",
    "\n",
    "- ***Duplicate data*** are two or more identical instances in a dataset. Duplicate instances are usually erroneous and should be removed.\n",
    "\n",
    "Missing, outlier, and duplicate data are collectively called ***dirty data***. A ***dirty instance*** and a ***dirty feature*** contain dirty data.\n",
    "\n",
    "Dirty data creates bias and inefficiencies in data analysis. Data scientists may struggle to interpret missing data. Values in erroneous duplicates appear too often and are weighted too heavily. Outliers skew results due to one potentially erroneous value. Consequently, missing, outlier, and duplicate data should be corrected or deleted.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Discarding data**\n",
    "\n",
    "Dirty data may be removed from a dataset by discarding instances, discarding features, or pairwise discarding.\n",
    "\n",
    "***Discarding instances***, also called ***listwise deletion*** or complete ***case removal***, removes dirty instances from the dataset. Dirty instances are usually discarded when:\n",
    "\n",
    "The dirty instances comprise a small percentage of the dataset.\n",
    "\n",
    "The dirty instances are random. When missing or outlier values are correlated with values in another feature, discarding dirty instances introduces bias.\n",
    "\n",
    "Instances are duplicates. Usually, duplicate instances are erroneous, and one instance should be discarded.\n",
    "\n",
    "***Discarding features*** removes dirty features that contain a high percentage of missing values, such as 60% or more. Discarding features does not usually apply to outlier data since, by definition, a small percentage of values can be outliers. Discarding features never applies to duplicate data.\n",
    "\n",
    "***Pairwise discarding*** retains dirty instances for some analyses and discards dirty instances for others. Instances are discarded only when an analysis uses a dirty feature. With pairwise discarding, the total number of instances varies for different analyses, which complicates comparisons and correlations. For this reason, pairwise discarding is not commonly used.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Imputing Data**\n",
    "Imputing data replaces missing and outlier data with new values. Imputing is more complex than discarding but retains all instances and features. Data may be imputed in several ways:\n",
    "\n",
    "- ***Hot-deck and cold-deck imputation*** replace missing and outlier data with a value from a randomly selected instance. In hot-deck imputation, the value is selected from other instances in the same dataset. In cold-deck imputation, the value is selected from a different dataset.\n",
    "\n",
    "- ***Mean imputation*** replaces missing and outlier data with the mean value of the feature. Missing and outlier data are excluded from the computation of the mean.\n",
    "\n",
    "- ***Regression imputation*** replaces missing and outlier data with a value computed from a regression model. In the regression model, the dependent variable is the dirty feature and the independent variables are other features. ***Stochastic regression imputation*** introduces uncertainty by adding or subtracting the regression variance to the new value. Regression models are discussed elsewhere in this material.\n",
    "\n",
    "Regression imputation is valuable if the dirty feature is highly correlated with other features. If not, mean imputation is commonly used. Hot- and cold-deck imputation were common when less computer power was available to compute mean and regression but are not widely used today.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cleaning data with Python**\n",
    "\n",
    "In Python, three special symbols represent missing values:\n",
    "\n",
    "- None represents any missing Python object, such as a string.\n",
    "\n",
    "- NaN represents a missing numeric value. NaN is a NumPy value, specified as numpy.NaN.\n",
    "\n",
    "- NaT represents a missing datetime value. NaT is a pandas value, specified as pandas.NaT.\n",
    "\n",
    "In addition, blank and 0 sometimes indicate a missing value, as in any tool.\n",
    "\n",
    "The pandas DataFrame class has methods that identify dirty data and discard and impute values. Important data cleaning methods are described in the table below. The table includes all required parameters and important optional parameters but excludes infrequently used optional parameters.\n",
    "\n",
    "Methods that change data contain an optional inplace parameter. If inplace is True, changes are made in the input dataframe. If inplace is False, changes are returned in a new dataframe.\n",
    "\n",
    "##### **pandas data cleaning methods.**\n",
    "**Method\tParameters\tDescription**\n",
    "***df.drop()***\t\n",
    "- labels=None, axis=0, inplace=False\t\n",
    "- Removes rows (axis=0) or columns (axis=1) from dataframe df. labels specifies the labels of rows or columns to drop.\n",
    "\n",
    "***df.drop_duplicates()***\t\n",
    "- subset=None, inplace=False\t\n",
    "- Removes duplicate rows from df. subset specifies the labels of columns used to identify duplicates. If subset=None, all columns are used.\n",
    "\n",
    "***df.dropna()***\t\n",
    "- axis=0, how='any', subset=None, inplace=False\t\n",
    "- Removes rows (axis=0) or columns (axis=1) containing missing values from df. subset specifies labels on the opposite axis to consider for missing values. how indicates whether to drop the row or column if any or if all values are missing.\n",
    "\n",
    "***df.duplicated()***\t\n",
    "- subset=None\t\n",
    "- Returns a Boolean series that identifies duplicate rows in df. true indicates a duplicate row. subset specifies the labels of columns used to identify duplicates. If subset=None, all columns are used.\n",
    "\n",
    "***df.fillna()***\t\n",
    "- value=None, inplace=False\t\n",
    "- Replaces NA and NaN values in df with value, which may be a scalar, dict, Series, or DataFrame.\n",
    "\n",
    "***df.isnull(), df.isna()***\t\n",
    "- none\t\n",
    "- Returns a dataframe of Boolean values. True in the returned dataframe indicates the corresponding value of the input df is None, NaT or NaN.\n",
    "\n",
    "***df.mean()***\t\n",
    "- axis=0, skip_na=True, numeric_only=None\t\n",
    "- Returns the mean values of rows (axis=0) or columns (axis=1) of df. skipna indicates whether to exclude unknown values in the calculation. numeric_only indicates whether to exclude non-numeric rows or columns.\n",
    "\n",
    "***df.replace()***\t\n",
    "- to_replace=None, value=NoDefault.no_default, inplace=False\t\n",
    "- Replaces to_replace values in df with value. to_replace and value may be str, dict, list, regex, or other data types.\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Enriching Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Appending data**\n",
    "Datasets can be enriched by appending new instances or features from external datasets. Leading sources of public datasets are described in the table below.\n",
    "\n",
    "##### **Leading public datasets.**\n",
    "\n",
    "**Name\tLink\tDescription**\n",
    "***Kaggle***\t\n",
    "- kaggle.com\t\n",
    "- Over 50,000 datasets on a broad range of subjects. Also provides Jupyter notebooks that analyze the datasets.\n",
    "\n",
    "***FiveThirtyEight***\t\n",
    "- data.fivethirtyeight.com\t\n",
    "- Datasets on politics, sports, science, economics, health, and culture, initially developed to support FiveThirtyEight publications.\n",
    "\n",
    "***University of California Irvine Machine Learning Repository***\t\n",
    "- archive.ics.uci.edu\t\n",
    "- 622 datasets, primarily in science, engineering, and business.\n",
    "\n",
    "***Data.gov***\t\n",
    "- data.gov\t\n",
    "- U.S. government datasets on agriculture, climate, energy, maritime, oceans, and health.\n",
    "\n",
    "***World Bank Open Data***\t\n",
    "- data.worldbank.org\t\n",
    "- Global datasets on subjects such as health, education, agriculture, and economics.\n",
    "\n",
    "***Nasdaq Data Link***\t\n",
    "- data.nasdaq.com\t\n",
    "- Financial and economic datasets.\n",
    "\n",
    "To append instances or features, prepare a subset of external data as follows:\n",
    "\n",
    "1. Identify the external dataset of interest.\n",
    "2. Identify a matching feature in the external and original datasets. The matching feature must uniquely identify instances of both datasets.\n",
    "3. Usually, only a subset of the external dataset is of interest. Extract the subset, including the matching feature.\n",
    "4. Structure and clean the subset, as described elsewhere in this material.\n",
    "\n",
    "To append instances, insert subset instances to the original dataset. To append features, merge subset instances with original instances using the matching feature, as illustrated in the animation below.\n",
    "\n",
    "Appending data may create missing data:\n",
    "\n",
    "- When appending instances, missing data is created if the two datasets have different features.\n",
    "- When appending features, missing data is created if instances of the two datasets do not match.\n",
    "\n",
    "Discard or impute the new missing data, as described elsewhere in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Enriching data with Python**\n",
    "\n",
    "pandas has many data enriching methods. Selected methods that append and derive data are described in the table below. The table includes all required parameters and important optional parameters but excludes infrequently used optional parameters.\n",
    "\n",
    "df.merge() emulates a relational database join. Relational joins merge two tables by specifying join columns in each table. The join columns correspond to the matching feature described in Appending data, above.\n",
    "\n",
    "A relational join merges rows that have matching join column values. Relational joins can be executed in several ways, including inner, outer, left, and right joins. These join types specify how to handle rows that do not have matching join column values. Inner, outer, left, and right joins are described in detail elsewhere in this material.\n",
    "\n",
    "##### **Python data enriching methods**\n",
    "\n",
    "**Method\tParameters\tDescription**\n",
    "\n",
    "***pd.concat()***\n",
    "- objs, axis=0, join='outer', ignore_index=False\t\n",
    "- Appends dataframes specified in objs parameter. Appends rows if axis=0 or columns if axis=1. join specifies whether to perform an 'outer' or 'inner' join. Resulting index values are unchanged if ignore_index=False or renumbered if ignore_index=True.\n",
    "\n",
    "***df.apply()***\t\n",
    "- func, axis=0\t\n",
    "- Applies the function specified in func parameter to a dataframe df. Applies function to each column if axis=0 or to each row if axis=1. Returns a Series or DataFrame.\n",
    "\n",
    "***df.insert()***\t\n",
    "- loc, column, value\t\n",
    "- Inserts a column to df. loc specifies the integer position of the new column. column specifies a string or numeric column label. value specifies column values as a Scalar or Series.\n",
    "\n",
    "***df.merge()***\t\n",
    "- right, how='inner', on=None, sort=False\t\n",
    "- Joins df with the right dataframe. how specifies whether to perform a 'left', 'right', 'outer', or 'inner' join. on specifies join column labels, which must appear in both dataframes. If on=None, all matching labels become join columns. sort=True sorts rows on the join columns.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
